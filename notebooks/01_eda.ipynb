{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5ac5d7e",
   "metadata": {},
   "source": [
    "# SentiSight - Exploratory Data Analysis\n",
    "\n",
    "This notebook performs exploratory data analysis on the customer feedback dataset.\n",
    "**Important:** The dataset is large, so we use chunked loading to avoid memory issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdab869",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0af4290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../')\n",
    "from src.preprocessing import DataLoader, TextPreprocessor\n",
    "\n",
    "# Configure warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85602260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure pandas display options\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.precision', 2)\n",
    "\n",
    "print(\"✓ Pandas display options configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309ae4eb",
   "metadata": {},
   "source": [
    "## 2. Load Dataset Information (without loading all data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0701dc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataLoader\n",
    "data_path = '../data/twcs.csv'\n",
    "loader = DataLoader(data_path, chunksize=5000)\n",
    "\n",
    "# Get dataset info without loading all data\n",
    "info = loader.get_info()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in info.items():\n",
    "    print(f\"{key:20s}: {value}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9ab03d",
   "metadata": {},
   "source": [
    "## 3. Load Sample Data for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aabb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a random sample (5000 rows) to avoid memory issues\n",
    "sample_size = 5000\n",
    "print(f\"Loading {sample_size} random samples from the dataset...\")\n",
    "df_sample = loader.load_sample(n_rows=sample_size)\n",
    "\n",
    "print(f\"\\n✓ Loaded {len(df_sample)} samples\")\n",
    "print(f\"Memory usage: {df_sample.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc78decf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"Sample Data Preview:\")\n",
    "df_sample.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee44407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and info\n",
    "print(\"Data Types and Info:\")\n",
    "df_sample.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0d0761",
   "metadata": {},
   "source": [
    "## 4. Data Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489326af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_data = pd.DataFrame({\n",
    "    'Column': df_sample.columns,\n",
    "    'Missing_Count': df_sample.isnull().sum().values,\n",
    "    'Missing_Percentage': (df_sample.isnull().sum().values / len(df_sample) * 100).round(2)\n",
    "})\n",
    "\n",
    "missing_data = missing_data[missing_data['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "if len(missing_data) > 0:\n",
    "    print(\"Missing Values Summary:\")\n",
    "    print(missing_data.to_string(index=False))\n",
    "else:\n",
    "    print(\"✓ No missing values found in the sample!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480bffbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"Statistical Summary:\")\n",
    "df_sample.describe(include='all').T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a90ead",
   "metadata": {},
   "source": [
    "## 5. Text Analysis\n",
    "\n",
    "We'll analyze the text column to understand content patterns, length distribution, and other text characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e51a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify text column (assuming it's named 'text' or similar)\n",
    "# Let's find the likely text column\n",
    "text_col = None\n",
    "for col in df_sample.columns:\n",
    "    if 'text' in col.lower() or 'message' in col.lower() or 'content' in col.lower() or 'tweet' in col.lower():\n",
    "        text_col = col\n",
    "        break\n",
    "\n",
    "if text_col is None:\n",
    "    # Use the column with longest average string length\n",
    "    str_cols = df_sample.select_dtypes(include=['object']).columns\n",
    "    avg_lengths = {col: df_sample[col].astype(str).str.len().mean() for col in str_cols}\n",
    "    text_col = max(avg_lengths, key=avg_lengths.get)\n",
    "\n",
    "print(f\"Text column identified: '{text_col}'\")\n",
    "print(f\"Sample text: {df_sample[text_col].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfafb7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate text statistics\n",
    "df_sample['text_length'] = df_sample[text_col].astype(str).str.len()\n",
    "df_sample['word_count'] = df_sample[text_col].astype(str).str.split().str.len()\n",
    "\n",
    "print(\"Text Statistics:\")\n",
    "print(f\"Average text length: {df_sample['text_length'].mean():.1f} characters\")\n",
    "print(f\"Average word count: {df_sample['word_count'].mean():.1f} words\")\n",
    "print(f\"Min text length: {df_sample['text_length'].min()}\")\n",
    "print(f\"Max text length: {df_sample['text_length'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a45622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(df_sample['text_length'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Text Length (characters)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Text Length')\n",
    "axes[0].axvline(df_sample['text_length'].mean(), color='red', linestyle='--', label=f'Mean: {df_sample[\"text_length\"].mean():.1f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Word count distribution\n",
    "axes[1].hist(df_sample['word_count'], bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1].set_xlabel('Word Count')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of Word Count')\n",
    "axes[1].axvline(df_sample['word_count'].mean(), color='red', linestyle='--', label=f'Mean: {df_sample[\"word_count\"].mean():.1f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989e6d6c",
   "metadata": {},
   "source": [
    "## 6. Feature Extraction Demo\n",
    "\n",
    "Let's extract features using our preprocessing module to get insights about text characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6581fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from a subset\n",
    "sample_subset = df_sample.head(100)\n",
    "features = sample_subset[text_col].apply(TextPreprocessor.extract_features)\n",
    "features_df = pd.DataFrame(features.tolist())\n",
    "\n",
    "print(\"Feature Extraction Sample:\")\n",
    "print(features_df.head())\n",
    "\n",
    "# Merge features\n",
    "df_with_features = pd.concat([sample_subset.reset_index(drop=True), features_df], axis=1)\n",
    "df_with_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307a7bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Urgency\n",
    "axes[0, 0].bar(['No Urgency', 'Has Urgency'], \n",
    "               [len(features_df[~features_df['has_urgency']]), \n",
    "                len(features_df[features_df['has_urgency']])],\n",
    "               color=['green', 'red'], alpha=0.7)\n",
    "axes[0, 0].set_title('Urgency Indicators')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "\n",
    "# Emotion counts\n",
    "emotion_data = pd.DataFrame({\n",
    "    'Negative': features_df['negative_emotion_count'],\n",
    "    'Positive': features_df['positive_emotion_count']\n",
    "})\n",
    "emotion_data.plot(kind='box', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Emotion Word Distribution')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "\n",
    "# Exclamation marks\n",
    "axes[1, 0].hist(features_df['exclamation_count'], bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_title('Exclamation Marks Distribution')\n",
    "axes[1, 0].set_xlabel('Count')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Capital letters ratio\n",
    "axes[1, 1].hist(features_df['caps_ratio'], bins=20, edgecolor='black', alpha=0.7, color='purple')\n",
    "axes[1, 1].set_title('Capital Letters Ratio')\n",
    "axes[1, 1].set_xlabel('Ratio')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c9c642",
   "metadata": {},
   "source": [
    "## 7. Save Findings\n",
    "\n",
    "Save key insights and statistics for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e96d549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of findings\n",
    "summary = {\n",
    "    'dataset_size': info['num_rows'],\n",
    "    'dataset_columns': info['num_columns'],\n",
    "    'sample_size': len(df_sample),\n",
    "    'text_column': text_col,\n",
    "    'avg_text_length': df_sample['text_length'].mean(),\n",
    "    'avg_word_count': df_sample['word_count'].mean(),\n",
    "    'has_urgency_pct': (features_df['has_urgency'].sum() / len(features_df) * 100),\n",
    "    'negative_emotion_avg': features_df['negative_emotion_count'].mean(),\n",
    "    'positive_emotion_avg': features_df['positive_emotion_count'].mean(),\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EDA SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for key, value in summary.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key:25s}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"{key:25s}: {value}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n✓ EDA Complete! Ready for model training.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
